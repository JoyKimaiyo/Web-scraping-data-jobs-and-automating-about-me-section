# ğŸ“ AI CV and Cover Letter Builder

Crafting a compelling cover letter and summary for a job application can be intimidating. It requires distilling your skills, experience, and personality into a concise narrative that stands out â€” especially in a world where companies use keyword-driven AI to screen candidates.

This project simplifies the process by:
- Scraping job data (Data Analyst, Data Scientist, Data Engineer, etc.) from **LinkedIn** using `web.py`
- Storing the data in a **MySQL** database
- Analyzing job descriptions using **Natural Language Processing (NLP)** to extract and weigh keywords
- Using **Gemini AI** in a **Streamlit** app to generate tailored cover letters and "About Me" sections

---

## ğŸ”§ Features

- ğŸ” Scrape LinkedIn job listings based on keyword, location, and filters
- ğŸ§  Extract key skills using NLP and TF-IDF
- âœ¨ Generate cover letters and About Me sections using **Gemini AI**
- ğŸ“Š Interactive UI built with **Streamlit**
- ğŸ’¾ Store job data in **MySQL** for reuse and analysis

---

## âš™ï¸ Configuration

Customize your scraping behavior by editing `config.json`:

```json
{
  "keywords": ["data analyst", "data scientist", "data engineer"],
  "locations": ["", "Remote", "Onsite", "Hybrid"],
  "date_range": "604800", 
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
  "languages": ["en"],
  "desc_words": ["senior", "principal"],
  "days_to_scrape": 7
}


## âš™ï¸ Configuration

Update your `config.json` with your preferred settings:

```json
{
  "keywords": ["data analyst", "data scientist", "data engineer"],
  "locations": ["", "Remote", "Onsite", "Hybrid"],
  "date_range": "604800", 
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
  "languages": ["en"],
  "desc_words": ["senior", "principal"],
  "days_to_scrape": 7
}
```

---

## ğŸ—ƒï¸ MySQL Setup

Create a MySQL database and table for storing scraped jobs:

```sql
-- Log in to MySQL
mysql -u your_username -p

-- Create database and table
CREATE DATABASE job_scraper;

USE job_scraper;

CREATE TABLE jobs (
    title TEXT,
    company TEXT,
    location TEXT,
    link TEXT,
    source TEXT,
    date_posted TEXT,
    work_type TEXT,
    employment_type TEXT,
    description TEXT
);
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/JoyKimaiyo/Web-scraping-data-jobs-and-automating-about-me-section
cd Web-scraping-data-jobs-and-automating-about-me-section
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the Scraper

```bash
python main.py
```

### 4. Launch the Streamlit App

```bash
streamlit run data_app.py
```

---

## ğŸ¤– Streamlit App Features

After scraping jobs, open the Streamlit app (`data_app.py`) to:

- âœ‰ï¸ Generate a **custom cover letter**
- ğŸ§â€â™‚ï¸ Auto-generate an **About Me** section
- ğŸ§  Powered by Gemini

---

## ğŸ“Œ Notes

- Use environment variables for storing your MySQL credentials securely
- LinkedIn may block frequent scrapers â€” implement delays and use responsibly
- This project is for educational and personal use only

---

## ğŸ§‘â€ğŸ’» Author

**Joy Kimaiyo**

- GitHub: [@JoyKimaiyo](https://github.com/JoyKimaiyo)
- Medium: [@joykimaiyo](https://medium.com/@joykimaiyo)

---
